# Qualitative Activity Recognition of Weight Lifting Exercises

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. 
In general, there are many potential applications for Human Activity Recognition, like elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The goal of the project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

### References
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

For more details, see http://groupware.les.inf.puc-rio.br/har

### Research question
The research question for this project can be expressed like:   

- Is it possible to predict the manner in which a person performs barbell from his/her movements?

In more detail: 

- Is it possible to predict the classe variable from accelerometers measures?


## 1. Data preprocessing

### Libraries
For this analysis, we will use the caret package (Classification And REgression Training) that contains a complete set of functions to create predictive models like data splitting, pre-processing, feature selection, training and testing models.
```{r, warning = FALSE, message = FALSE}
library(caret)
```

### Data loading
The training and test data used for the project are available at  

- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv 
- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv 

respectively.
 
After downloading the two files, we load them into two R data sets.
```{r}
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")
```
### Data cleaning
The training set contains `r nrow(train)` observations but there are 67 variables containing 19216 NA values and other 33 variables containing 19216 empty strings.
```{r}
t <- table(colSums(is.na(train)))
t
table(colSums(train == ""))
```
```{r, echo = FALSE}
perc = as.integer(names(t)[2])/nrow(train)
```

It is possible to ignore these features because the `r perc`% of their values are not significant. Other features in the dataset are not measurements, like X, user_name,	raw_timestamp_part_1,	raw_timestamp_part_2,	cvtd_timestamp and new_window and num_window, and so they can be ignored too. All these features have so been removed from the training dataset.
```{r}
index <- which(colSums(is.na(train)) == 0 & colSums(train == "") == 0)
train <- train[,index] # remove features with NA and "" values
train <- train[, 7:60] # remove non significant features
```

## 2. Model fitting
### Cross Validation 
The cross validation technique is used to limit problems like overfitting and obtain a high out of sample accuracy and consequently a low out of sample error (we will use the accuracy as error measure). Therefore, the training dataset is partitioned in a training and a test set using the rule of thumb of 60/40%. Then the model is built on this training set and finally evaluated on the test set. 
```{r}
set.seed(8503)
partition <- createDataPartition(y = train$classe, p = 0.6, list = FALSE)
training <- train[partition, ]
testing <- train[-partition, ]
```
### Model training
The model for this data set is built using the random forest algorithm because it can manage very well data with a great amount of features reaching at the same time a very high accuracy. The trControl function is used with cross validation and parallel processing.
```{r, cache = TRUE, warning = FALSE, message = FALSE}
set.seed(4567)
trControl = trainControl(method = "cv", number = 4, allowParallel = TRUE)
model <- train(classe ~ ., data = training, method = "rf", prox = TRUE, 
              trControl = trControl)
model
```
It is possible to see that the accuracy of the model chosen is 100%, a very accurate model. This is the in sample accuracy, the accuracy obtained on the same data the model is built on, and it is always optimistic. More important is the out of sample accuracy, the accuracy on a new data set like a test set, that is expected to be generally lower than the in sample accuracy due to overfitting. Cross validation is used to avoid overfitting and obtain a good out of sample accuracy too.

### Model evaluation
Prediction and evaluation are made on the test set giving an accuracy of 99.7% that is an extremely good level of accuracy as we expected from cross-validation.
```{r}
pred <- predict(model, testing)
confusionMatrix(pred, testing$classe)
```



