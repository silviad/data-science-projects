# Human Activity Recognition

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. 
In general, there are many potential applications for Human Activity Recognition, like elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The goal of the project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

### References
For more details see http://groupware.les.inf.puc-rio.br/har

### Research question
The research question for this project can be expressed like:   

- Is it possible to predict the manner in which a person perform barbell from his/her movements?

In more details: 

- Is it possible to predict the classe variable from accelerometers measures variables?


## 1. Data preprocessing

### Libraries
For this analysis, we will use the caret package (Classification And REgression Training) that contains a complete set of functions to create predictive models like data splitting, pre-processing, feature selection, training and testing models.
```{r, warning = FALSE, message = FALSE}
library(caret)
```
```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(randomForest)
```
### Data loading
The training and test data used for the project are available at  

- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv 
- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv 

respectively.
 
After downloading the two files, we read them and load two R data sets.
```{r}
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")
```
### Data cleaning
In the data set there are 67 variables contain 19216 NA values and other 33 variables contain 19216 empty strings.
```{r}
t <- table(colSums(is.na(train)))
t
table(colSums(train == ""))
```
```{r, echo = FALSE}
perc = as.integer(names(t)[2])/nrow(train)
```

It is possible to ignore these features because the `r perc`% of their values are not significant. Other features in the dataset are not measurements, like X, user_name,	raw_timestamp_part_1,	raw_timestamp_part_2,	cvtd_timestamp and new_window and num_window, and so they can be ignored too. All these features have so been removed from the dataset.
```{r}
index <- which(colSums(is.na(train)) == 0 & colSums(train == "") == 0)
train <- train[,index] # remove features with NA and "" values
train <- train[, 7:60] # remove non significant features
```

## 2. Model fitting
### Cross Validation 
Cross validation technique is used to limit problems like overfitting and obtain a high out of sample accuracy and consequently a low out of sample error (we will use accuracy as error measure). Therefore, first the training dataset is partitioned in a training and a test set using the rule of thumb of 70/30%. Then the model is built on this training set and finally evaluated on the test set. 
```{r}
set.seed(8503)
partition <- createDataPartition(y = train$classe, p = 0.6, list = FALSE)
training <- train[partition, ]
testing <- train[-partition, ]
```
### Model training
The model for this analysis is built using the random forest algorithm because it can manage very well data with a great amount of features reaching at the same time a very high accuracy. The trControl function is used with cross validation and parallel processing.
```{r, cache = TRUE, warning = FALSE, message = FALSE}
set.seed(4567)
trControl = trainControl(method = "cv", number = 4, allowParallel = TRUE)
model <- train(classe ~ ., data = training, method = "rf", prox = TRUE, 
              trControl = trControl)
model
```
It is possible to see that the accuracy of the model chosen is 100%, a very accurate model. This is the in sample accuracy, the accuracy obtained on the same data the model is built, and it is always optimistic. More important is the out of sample accuracy, the accuracy on a new data set, a test set to predict, that is expected to be generally lower than the in sample accuracy due to overfitting. Cross validation is used to avoid overfitting and obtain a good out of sample accuracy too.

### Model evaluation
Prediction and evaluation are made on the test set giving an accuracy of 99.7% that is an extremely good level of accuracy as we expected from cross-validation.
```{r}
pred <- predict(model, testing)
confusionMatrix(pred, testing$classe)
```



