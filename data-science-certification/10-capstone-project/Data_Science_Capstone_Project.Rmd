# Predictive text Shiny App

## Synopsis

Mobile devices like smartphones and tablets are already part of daily life of many people but typing on mobile keyboards can be sometimes frustrating. The goal of this project is to develop a predictive text model like SwiftKey and integrate it on a Shiny App available online to make typing easy and life better for the users.

The key idea of the solution developed here is to apply a well-known best practice of user-experience design to predictive text models: context matters. In other words, the Shiny App will adapt not only to the user as the current smart devices usually do but also to the context in order to better target the prediction and improve typing experience. 


## Details


### The Data

The data for the project is a corpus of three documents containing not formatted English text. 
This text have been crawled from three different public web sources (Twitter, blogs and news articles) and saved in three different files.
The files can be downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

**Basic summaries**

The summaries below show the number of lines, the number of words and some text of example for each of the three files.
```{r, echo = FALSE, chache = TRUE, warning = FALSE, message = FALSE} 
source("load.R")
load.libraries.functions()
```

- Twitter file  
```{r, echo = FALSE, chache = TRUE} 
basic.summaries.file("twitter.txt")
```
- Blog file  
```{r, echo = FALSE, chache = TRUE} 
basic.summaries.file("blogs.txt")
```
- News file  
```{r, echo = FALSE, chache = TRUE} 
basic.summaries.file("news.txt")
```
**Text processing**

A random sample has been extracted from each file to make exploratory analysis. After sampling, the text has been cleaned using open source libraries (R packages). Punctuation, numbers, ["stop words"](http://en.wikipedia.org/wiki/Stop_words), "bad words" (words with offensive and profane meaning, see [profanity filter](http://en.wikipedia.org/wiki/Wordfilter#Removal_of_vulgar_language)) and extra white spaces have been removed. The text has been divided in sentences and words.


### Exploratory analysis

It has been expected there are obvious linguistic differences between the files because their text has been written in completely different contexts: Twitter more informal with more slang and abbreviations, blogs and news more formal with longer sentences and less spelling errors. 

**N-gram distributions**

From the plots below, it is possible to see the frequency of the most frequent sequences of 1, 2 and 3 words extracted from the three files, called n-grams (for more details about n-gram, see [n-gram](http://en.wikipedia.org/wiki/N-gram)). The distributions are different for the three files and the difference is bigger the higher is the number of words in the sequence. The relative summary statistics for n-grams are available in the appendix (Summary statistics).

```{r, echo = FALSE, chache=TRUE, warning=FALSE, message=FALSE, fig.height=5, fig.width=10} 
#size <- 50
size <- 10000
tdm.list.twitter <- prepare.corpus("twitter.txt", size)
par(mar=c(5.1,10,4.1,2.1), mfrow=c(1,3), oma = c(0, 0, 2, 0))
for (n in 1:3) display.barplot(tdm.list.twitter[[n]], n, "yellow")
mtext("Twitter", outer = TRUE, cex = 1.5)
tdm.list.blogs <- prepare.corpus("blogs.txt", size)
for (n in 1:3) display.barplot(tdm.list.blogs[[n]], n, "green")
mtext("Blog", outer = TRUE, cex = 1.5)
tdm.list.news <- prepare.corpus("news.txt", size)
for (n in 1:3) display.barplot(tdm.list.news[[n]], n, "purple")
mtext("News", outer = TRUE, cex = 1.5)
```

**Unigram Clouds**

In addition, the word clouds below show clearly the different dictionaries used in the different contexts. To interpret the plot, the following clues have to be applied: words with the same frequency have the same color, the more frequent a word is, the closer to the center of the cloud and the larger the font.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=20}
l <- list(tdm.list.twitter[[1]], tdm.list.blogs[[1]], tdm.list.news[[1]])
display.clouds(l)
```

**Text length distributions**

The histograms below show the distribution of text length measured in number of characters and in number of words for the three files. The distribution are completely different as expected: shorter text with few words for Twitter, longer text with more words for the news and the blogs.

```{r, echo = FALSE, chache=TRUE, warning=FALSE, message=FALSE, fig.height=5, fig.width=10}
par(mar=c(5.1,4.1,4.1,2.1), mfcol=c(2,3))
display.histogram("sample.twitter.txt")
display.histogram("sample.blogs.txt")
display.histogram("sample.news.txt")
```

**Stylistics analysis**

A stylistics analysis has been conducted using a [clustering technique](http://en.wikipedia.org/wiki/Cluster_analysis). The analysis has detected the difference between Twitter data and the other two sources grouping them in different clusters, as it is possible to see form the plot in the appendix (Stylistics analysis).

**Conclusion**

To summarize, context matters. The dictionary and style used are different in different contexts and in most cases also if the user is the same.

### Predictive Model and Shiny App
Recent algorithms for natural language processing [NLP](http://en.wikipedia.org/wiki/Natural_language_processing) have been based on [machine learning](http://en.wikipedia.org/wiki/Machine_learning) techniques. Therefore, the predictive model of this project has been built using the best practices of these techniques (for more details, see [machine learning course](https://WWW.coursera.org/course/ml)). The approach can be summarized in the following steps:

1. Usually, data are more important than algorithm and often, "it's not who has the best algorithm that wins. It's who has the most data" [prof Andrew Ng]. Therefore, first prepare a huge data set reaching a trade-off with memory limits. Unlike exploratory analysis, in a text predictive model every word has its own importance so it is better not to apply any filter like "stop words" filter. 

2. Start with a simple algorithm implemented quickly and test it. In this case, the algorithm is based on n-grams backoff techniques ([backoff](http://en.wikipedia.org/wiki/Katz%27s_back-off_model)). In addition, the model includes three predictive models, one for each different context.  

3. Improve the algorithm applying advanced techniques to refine the prediction and get a better accuracy. The additional techniques chosen are:  managing unseen words ([smoothing](http://en.wikipedia.org/wiki/N-gram#Smoothing_techniques)), spelling correction, dictionary matching and grammatical disambiguation ([part of speech tagging](http://en.wikipedia.org/wiki/Part-of-speech_tagging)).

The Shiny App will give the user a powerful tool to write quicker than usual on a keyboard device. The application implements sentence completion and spell checking services and at the same time gives the user the possibility to switch between different contexts in order to make the algorithm learn faster, adapt better to different typing style of the user and eventually offer a better typing experience. 


## Appendix

**Summary statistics** 

- Twitter file
```{r, echo = FALSE, chache = TRUE, warning = FALSE, message = FALSE}  
for (n in 1:3) display.summaries(tdm.list.twitter[[n]], n)
```   
- Blog file
```{r, echo = FALSE, chache = TRUE, warning = FALSE, message = FALSE}  
for (n in 1:3) display.summaries(tdm.list.blogs[[n]], n)
``` 
- News file
```{r, echo = FALSE, chache = TRUE, warning = FALSE, message = FALSE}  
for (n in 1:3) display.summaries(tdm.list.news [[n]], n)
``` 

**Stylistics analysis**

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}  
res <- stylo()
```

